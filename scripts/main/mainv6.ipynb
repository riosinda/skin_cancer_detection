{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32     # number of images to be processed in a batch\n",
    "NUM_EPOCHS = 200     # number of times the entire dataset is passed through the network\n",
    "NUM_CLASSES = 1     # number of classes in the dataset\n",
    "t = 224             # target size of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the InceptionV3 model\n",
    "base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(t, t, 3))\n",
    "\n",
    "# Freeze the pretrained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added layer to base model\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "predictions = keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we will train\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.001)      # Adam optimizer\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    staircase=True\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loos_fn = keras.losses.BinaryCrossentropy()                 # Binary crossentropy loss function\n",
    "model.compile(optimizer=optimizer, loss=loos_fn, metrics=['accuracy'])  # Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipo = \"mirror\"\n",
    "tipo = \"black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded 6518\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "dataTr = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/train/bcc/\",'*.jpg')):\n",
    "    dataTr.append([1, cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC)])\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/train/scc/\",'*.jpg')):\n",
    "    dataTr.append([0, cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC)])\n",
    "\n",
    "print('Images loaded', len(dataTr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "for i, j in dataTr:\n",
    "    x_train.append(j)\n",
    "    y_train.append(i)\n",
    "x_train = np.array(x_train)/255 # Normalize\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded 2794\n",
      "Labels loaded 2794\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/validation/bcc/\",'*.jpg')):\n",
    "    x_test.append(cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC))\n",
    "    y_test.append(1)\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/validation/scc/\",'*.jpg')):\n",
    "    x_test.append(cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC))\n",
    "    y_test.append(0)\n",
    "\n",
    "print('Images loaded', len(x_test))\n",
    "print('Labels loaded', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)/255 # Normalize\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 100,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'best_cnn1_MobilNet.h5'\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 2.7214 - accuracy: 0.7289\n",
      "Epoch 1: val_loss improved from inf to 0.92065, saving model to best_cnn1_MobilNet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 250s 1s/step - loss: 2.7214 - accuracy: 0.7289 - val_loss: 0.9206 - val_accuracy: 0.7448\n",
      "Epoch 2/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.7351 - accuracy: 0.7490\n",
      "Epoch 2: val_loss improved from 0.92065 to 0.59624, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 118s 577ms/step - loss: 0.7351 - accuracy: 0.7490 - val_loss: 0.5962 - val_accuracy: 0.7867\n",
      "Epoch 3/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7686\n",
      "Epoch 3: val_loss improved from 0.59624 to 0.56607, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 119s 583ms/step - loss: 0.5981 - accuracy: 0.7686 - val_loss: 0.5661 - val_accuracy: 0.7727\n",
      "Epoch 4/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5704 - accuracy: 0.7668\n",
      "Epoch 4: val_loss improved from 0.56607 to 0.53664, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.5704 - accuracy: 0.7668 - val_loss: 0.5366 - val_accuracy: 0.7792\n",
      "Epoch 5/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.7579\n",
      "Epoch 5: val_loss improved from 0.53664 to 0.52043, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 126s 617ms/step - loss: 0.5587 - accuracy: 0.7579 - val_loss: 0.5204 - val_accuracy: 0.7838\n",
      "Epoch 6/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.7616\n",
      "Epoch 6: val_loss improved from 0.52043 to 0.51466, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.5483 - accuracy: 0.7616 - val_loss: 0.5147 - val_accuracy: 0.7702\n",
      "Epoch 7/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7668\n",
      "Epoch 7: val_loss improved from 0.51466 to 0.50559, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 126s 616ms/step - loss: 0.5394 - accuracy: 0.7668 - val_loss: 0.5056 - val_accuracy: 0.7992\n",
      "Epoch 8/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.7771\n",
      "Epoch 8: val_loss improved from 0.50559 to 0.50172, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 606ms/step - loss: 0.5242 - accuracy: 0.7771 - val_loss: 0.5017 - val_accuracy: 0.7895\n",
      "Epoch 9/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.7763\n",
      "Epoch 9: val_loss improved from 0.50172 to 0.49305, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 124s 610ms/step - loss: 0.5196 - accuracy: 0.7763 - val_loss: 0.4931 - val_accuracy: 0.7917\n",
      "Epoch 10/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7803\n",
      "Epoch 10: val_loss improved from 0.49305 to 0.48243, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 125s 613ms/step - loss: 0.5093 - accuracy: 0.7803 - val_loss: 0.4824 - val_accuracy: 0.8003\n",
      "Epoch 11/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.7757\n",
      "Epoch 11: val_loss did not improve from 0.48243\n",
      "204/204 [==============================] - 126s 619ms/step - loss: 0.5126 - accuracy: 0.7757 - val_loss: 0.4989 - val_accuracy: 0.7799\n",
      "Epoch 12/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.7864\n",
      "Epoch 12: val_loss improved from 0.48243 to 0.47582, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.5018 - accuracy: 0.7864 - val_loss: 0.4758 - val_accuracy: 0.7999\n",
      "Epoch 13/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7834\n",
      "Epoch 13: val_loss improved from 0.47582 to 0.47131, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4985 - accuracy: 0.7834 - val_loss: 0.4713 - val_accuracy: 0.8060\n",
      "Epoch 14/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.7861\n",
      "Epoch 14: val_loss improved from 0.47131 to 0.46443, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4946 - accuracy: 0.7861 - val_loss: 0.4644 - val_accuracy: 0.8193\n",
      "Epoch 15/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7890\n",
      "Epoch 15: val_loss did not improve from 0.46443\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4954 - accuracy: 0.7890 - val_loss: 0.4714 - val_accuracy: 0.8049\n",
      "Epoch 16/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.7920\n",
      "Epoch 16: val_loss improved from 0.46443 to 0.46433, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4856 - accuracy: 0.7920 - val_loss: 0.4643 - val_accuracy: 0.8117\n",
      "Epoch 17/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.7904\n",
      "Epoch 17: val_loss did not improve from 0.46433\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4888 - accuracy: 0.7904 - val_loss: 0.4709 - val_accuracy: 0.8121\n",
      "Epoch 18/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.7930\n",
      "Epoch 18: val_loss did not improve from 0.46433\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4870 - accuracy: 0.7930 - val_loss: 0.4933 - val_accuracy: 0.7810\n",
      "Epoch 19/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.7976\n",
      "Epoch 19: val_loss improved from 0.46433 to 0.45805, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4775 - accuracy: 0.7976 - val_loss: 0.4581 - val_accuracy: 0.8210\n",
      "Epoch 20/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.7979\n",
      "Epoch 20: val_loss did not improve from 0.45805\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4786 - accuracy: 0.7979 - val_loss: 0.4583 - val_accuracy: 0.8150\n",
      "Epoch 21/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.7975\n",
      "Epoch 21: val_loss improved from 0.45805 to 0.45396, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4805 - accuracy: 0.7975 - val_loss: 0.4540 - val_accuracy: 0.8128\n",
      "Epoch 22/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8019\n",
      "Epoch 22: val_loss did not improve from 0.45396\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4752 - accuracy: 0.8019 - val_loss: 0.4582 - val_accuracy: 0.8121\n",
      "Epoch 23/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4779 - accuracy: 0.7970\n",
      "Epoch 23: val_loss did not improve from 0.45396\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4779 - accuracy: 0.7970 - val_loss: 0.4550 - val_accuracy: 0.8089\n",
      "Epoch 24/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.7998\n",
      "Epoch 24: val_loss improved from 0.45396 to 0.45376, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4719 - accuracy: 0.7998 - val_loss: 0.4538 - val_accuracy: 0.8293\n",
      "Epoch 25/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.7999\n",
      "Epoch 25: val_loss did not improve from 0.45376\n",
      "204/204 [==============================] - 126s 616ms/step - loss: 0.4690 - accuracy: 0.7999 - val_loss: 0.4567 - val_accuracy: 0.8200\n",
      "Epoch 26/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.8019\n",
      "Epoch 26: val_loss did not improve from 0.45376\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4699 - accuracy: 0.8019 - val_loss: 0.4561 - val_accuracy: 0.8175\n",
      "Epoch 27/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.8013\n",
      "Epoch 27: val_loss improved from 0.45376 to 0.44435, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4699 - accuracy: 0.8013 - val_loss: 0.4444 - val_accuracy: 0.8207\n",
      "Epoch 28/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.7998\n",
      "Epoch 28: val_loss did not improve from 0.44435\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4767 - accuracy: 0.7998 - val_loss: 0.4464 - val_accuracy: 0.8239\n",
      "Epoch 29/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.8078\n",
      "Epoch 29: val_loss improved from 0.44435 to 0.44252, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4647 - accuracy: 0.8078 - val_loss: 0.4425 - val_accuracy: 0.8218\n",
      "Epoch 30/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.8065\n",
      "Epoch 30: val_loss did not improve from 0.44252\n",
      "204/204 [==============================] - 124s 607ms/step - loss: 0.4626 - accuracy: 0.8065 - val_loss: 0.4476 - val_accuracy: 0.8214\n",
      "Epoch 31/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.8053\n",
      "Epoch 31: val_loss did not improve from 0.44252\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4673 - accuracy: 0.8053 - val_loss: 0.4472 - val_accuracy: 0.8185\n",
      "Epoch 32/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.8090\n",
      "Epoch 32: val_loss improved from 0.44252 to 0.43811, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4589 - accuracy: 0.8090 - val_loss: 0.4381 - val_accuracy: 0.8185\n",
      "Epoch 33/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4668 - accuracy: 0.8006\n",
      "Epoch 33: val_loss did not improve from 0.43811\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4668 - accuracy: 0.8006 - val_loss: 0.4598 - val_accuracy: 0.8125\n",
      "Epoch 34/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.8096\n",
      "Epoch 34: val_loss did not improve from 0.43811\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4586 - accuracy: 0.8096 - val_loss: 0.4492 - val_accuracy: 0.8121\n",
      "Epoch 35/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.8056\n",
      "Epoch 35: val_loss improved from 0.43811 to 0.43657, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4586 - accuracy: 0.8056 - val_loss: 0.4366 - val_accuracy: 0.8300\n",
      "Epoch 36/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.8128\n",
      "Epoch 36: val_loss did not improve from 0.43657\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4540 - accuracy: 0.8128 - val_loss: 0.4423 - val_accuracy: 0.8178\n",
      "Epoch 37/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.8096\n",
      "Epoch 37: val_loss did not improve from 0.43657\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4543 - accuracy: 0.8096 - val_loss: 0.4607 - val_accuracy: 0.8028\n",
      "Epoch 38/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.8071\n",
      "Epoch 38: val_loss did not improve from 0.43657\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4562 - accuracy: 0.8071 - val_loss: 0.4445 - val_accuracy: 0.8246\n",
      "Epoch 39/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.8118\n",
      "Epoch 39: val_loss did not improve from 0.43657\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4535 - accuracy: 0.8118 - val_loss: 0.4544 - val_accuracy: 0.8164\n",
      "Epoch 40/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8162\n",
      "Epoch 40: val_loss improved from 0.43657 to 0.43392, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4493 - accuracy: 0.8162 - val_loss: 0.4339 - val_accuracy: 0.8182\n",
      "Epoch 41/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.8141\n",
      "Epoch 41: val_loss did not improve from 0.43392\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4488 - accuracy: 0.8141 - val_loss: 0.4412 - val_accuracy: 0.8150\n",
      "Epoch 42/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.8216\n",
      "Epoch 42: val_loss improved from 0.43392 to 0.43387, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4488 - accuracy: 0.8216 - val_loss: 0.4339 - val_accuracy: 0.8243\n",
      "Epoch 43/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.8085\n",
      "Epoch 43: val_loss did not improve from 0.43387\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4572 - accuracy: 0.8085 - val_loss: 0.4461 - val_accuracy: 0.8168\n",
      "Epoch 44/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.8213\n",
      "Epoch 44: val_loss improved from 0.43387 to 0.42941, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4463 - accuracy: 0.8213 - val_loss: 0.4294 - val_accuracy: 0.8379\n",
      "Epoch 45/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8179\n",
      "Epoch 45: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 125s 612ms/step - loss: 0.4479 - accuracy: 0.8179 - val_loss: 0.4320 - val_accuracy: 0.8250\n",
      "Epoch 46/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8144\n",
      "Epoch 46: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4536 - accuracy: 0.8144 - val_loss: 0.4413 - val_accuracy: 0.8189\n",
      "Epoch 47/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.8119\n",
      "Epoch 47: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4524 - accuracy: 0.8119 - val_loss: 0.4578 - val_accuracy: 0.8021\n",
      "Epoch 48/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.8217\n",
      "Epoch 48: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4437 - accuracy: 0.8217 - val_loss: 0.4369 - val_accuracy: 0.8236\n",
      "Epoch 49/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.8170\n",
      "Epoch 49: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4515 - accuracy: 0.8170 - val_loss: 0.4431 - val_accuracy: 0.8200\n",
      "Epoch 50/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8243\n",
      "Epoch 50: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4410 - accuracy: 0.8243 - val_loss: 0.4347 - val_accuracy: 0.8232\n",
      "Epoch 51/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8211\n",
      "Epoch 51: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4493 - accuracy: 0.8211 - val_loss: 0.4482 - val_accuracy: 0.8114\n",
      "Epoch 52/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.8200\n",
      "Epoch 52: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 123s 606ms/step - loss: 0.4428 - accuracy: 0.8200 - val_loss: 0.4380 - val_accuracy: 0.8243\n",
      "Epoch 53/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8164\n",
      "Epoch 53: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4475 - accuracy: 0.8164 - val_loss: 0.4347 - val_accuracy: 0.8228\n",
      "Epoch 54/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.8222\n",
      "Epoch 54: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 126s 617ms/step - loss: 0.4401 - accuracy: 0.8222 - val_loss: 0.4351 - val_accuracy: 0.8203\n",
      "Epoch 55/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.8188\n",
      "Epoch 55: val_loss did not improve from 0.42941\n",
      "204/204 [==============================] - 126s 617ms/step - loss: 0.4406 - accuracy: 0.8188 - val_loss: 0.4329 - val_accuracy: 0.8257\n",
      "Epoch 56/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8196\n",
      "Epoch 56: val_loss improved from 0.42941 to 0.42888, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4392 - accuracy: 0.8196 - val_loss: 0.4289 - val_accuracy: 0.8253\n",
      "Epoch 57/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8222\n",
      "Epoch 57: val_loss did not improve from 0.42888\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4400 - accuracy: 0.8222 - val_loss: 0.4533 - val_accuracy: 0.8082\n",
      "Epoch 58/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8185\n",
      "Epoch 58: val_loss did not improve from 0.42888\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4453 - accuracy: 0.8185 - val_loss: 0.4350 - val_accuracy: 0.8228\n",
      "Epoch 59/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.8185\n",
      "Epoch 59: val_loss did not improve from 0.42888\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4472 - accuracy: 0.8185 - val_loss: 0.4366 - val_accuracy: 0.8189\n",
      "Epoch 60/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8260\n",
      "Epoch 60: val_loss improved from 0.42888 to 0.42617, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4335 - accuracy: 0.8260 - val_loss: 0.4262 - val_accuracy: 0.8253\n",
      "Epoch 61/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.8147\n",
      "Epoch 61: val_loss did not improve from 0.42617\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4488 - accuracy: 0.8147 - val_loss: 0.4268 - val_accuracy: 0.8271\n",
      "Epoch 62/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.8171\n",
      "Epoch 62: val_loss did not improve from 0.42617\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4406 - accuracy: 0.8171 - val_loss: 0.4401 - val_accuracy: 0.8171\n",
      "Epoch 63/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.8243\n",
      "Epoch 63: val_loss improved from 0.42617 to 0.41945, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4352 - accuracy: 0.8243 - val_loss: 0.4195 - val_accuracy: 0.8350\n",
      "Epoch 64/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8207\n",
      "Epoch 64: val_loss did not improve from 0.41945\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4440 - accuracy: 0.8207 - val_loss: 0.4291 - val_accuracy: 0.8296\n",
      "Epoch 65/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.8211\n",
      "Epoch 65: val_loss did not improve from 0.41945\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4364 - accuracy: 0.8211 - val_loss: 0.4462 - val_accuracy: 0.8142\n",
      "Epoch 66/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4359 - accuracy: 0.8280\n",
      "Epoch 66: val_loss improved from 0.41945 to 0.41755, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4359 - accuracy: 0.8280 - val_loss: 0.4176 - val_accuracy: 0.8314\n",
      "Epoch 67/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.8177\n",
      "Epoch 67: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4462 - accuracy: 0.8177 - val_loss: 0.4267 - val_accuracy: 0.8268\n",
      "Epoch 68/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.8237\n",
      "Epoch 68: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4375 - accuracy: 0.8237 - val_loss: 0.4297 - val_accuracy: 0.8278\n",
      "Epoch 69/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.8337\n",
      "Epoch 69: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4315 - accuracy: 0.8337 - val_loss: 0.4282 - val_accuracy: 0.8221\n",
      "Epoch 70/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4382 - accuracy: 0.8222\n",
      "Epoch 70: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4382 - accuracy: 0.8222 - val_loss: 0.4288 - val_accuracy: 0.8289\n",
      "Epoch 71/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.8262\n",
      "Epoch 71: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4368 - accuracy: 0.8262 - val_loss: 0.4268 - val_accuracy: 0.8286\n",
      "Epoch 72/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.8292\n",
      "Epoch 72: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4322 - accuracy: 0.8292 - val_loss: 0.4282 - val_accuracy: 0.8261\n",
      "Epoch 73/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.8200\n",
      "Epoch 73: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4417 - accuracy: 0.8200 - val_loss: 0.4176 - val_accuracy: 0.8357\n",
      "Epoch 74/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8228\n",
      "Epoch 74: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4361 - accuracy: 0.8228 - val_loss: 0.4387 - val_accuracy: 0.8182\n",
      "Epoch 75/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.8173\n",
      "Epoch 75: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4454 - accuracy: 0.8173 - val_loss: 0.4332 - val_accuracy: 0.8261\n",
      "Epoch 76/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8277\n",
      "Epoch 76: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4347 - accuracy: 0.8277 - val_loss: 0.4318 - val_accuracy: 0.8268\n",
      "Epoch 77/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8242\n",
      "Epoch 77: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4367 - accuracy: 0.8242 - val_loss: 0.4323 - val_accuracy: 0.8228\n",
      "Epoch 78/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.8282\n",
      "Epoch 78: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4362 - accuracy: 0.8282 - val_loss: 0.4408 - val_accuracy: 0.8150\n",
      "Epoch 79/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.8157\n",
      "Epoch 79: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4448 - accuracy: 0.8157 - val_loss: 0.4321 - val_accuracy: 0.8307\n",
      "Epoch 80/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.8256\n",
      "Epoch 80: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4362 - accuracy: 0.8256 - val_loss: 0.4201 - val_accuracy: 0.8336\n",
      "Epoch 81/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.8254\n",
      "Epoch 81: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4421 - accuracy: 0.8254 - val_loss: 0.4343 - val_accuracy: 0.8296\n",
      "Epoch 82/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8305\n",
      "Epoch 82: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4303 - accuracy: 0.8305 - val_loss: 0.4210 - val_accuracy: 0.8325\n",
      "Epoch 83/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.8246\n",
      "Epoch 83: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4375 - accuracy: 0.8246 - val_loss: 0.4431 - val_accuracy: 0.8168\n",
      "Epoch 84/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4311 - accuracy: 0.8272\n",
      "Epoch 84: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4311 - accuracy: 0.8272 - val_loss: 0.4252 - val_accuracy: 0.8278\n",
      "Epoch 85/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.8282\n",
      "Epoch 85: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4313 - accuracy: 0.8282 - val_loss: 0.4259 - val_accuracy: 0.8257\n",
      "Epoch 86/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8254\n",
      "Epoch 86: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4316 - accuracy: 0.8254 - val_loss: 0.4183 - val_accuracy: 0.8346\n",
      "Epoch 87/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.8265\n",
      "Epoch 87: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4334 - accuracy: 0.8265 - val_loss: 0.4272 - val_accuracy: 0.8311\n",
      "Epoch 88/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.8266\n",
      "Epoch 88: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4329 - accuracy: 0.8266 - val_loss: 0.4239 - val_accuracy: 0.8311\n",
      "Epoch 89/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.8239\n",
      "Epoch 89: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4344 - accuracy: 0.8239 - val_loss: 0.4354 - val_accuracy: 0.8271\n",
      "Epoch 90/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.8228\n",
      "Epoch 90: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4387 - accuracy: 0.8228 - val_loss: 0.4294 - val_accuracy: 0.8214\n",
      "Epoch 91/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.8274\n",
      "Epoch 91: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4349 - accuracy: 0.8274 - val_loss: 0.4298 - val_accuracy: 0.8304\n",
      "Epoch 92/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8260\n",
      "Epoch 92: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4394 - accuracy: 0.8260 - val_loss: 0.4322 - val_accuracy: 0.8289\n",
      "Epoch 93/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8262\n",
      "Epoch 93: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4384 - accuracy: 0.8262 - val_loss: 0.4258 - val_accuracy: 0.8286\n",
      "Epoch 94/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4341 - accuracy: 0.8325\n",
      "Epoch 94: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4341 - accuracy: 0.8325 - val_loss: 0.4210 - val_accuracy: 0.8304\n",
      "Epoch 95/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4358 - accuracy: 0.8260\n",
      "Epoch 95: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4358 - accuracy: 0.8260 - val_loss: 0.4436 - val_accuracy: 0.8139\n",
      "Epoch 96/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.8303\n",
      "Epoch 96: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4334 - accuracy: 0.8303 - val_loss: 0.4254 - val_accuracy: 0.8200\n",
      "Epoch 97/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8279\n",
      "Epoch 97: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4390 - accuracy: 0.8279 - val_loss: 0.4217 - val_accuracy: 0.8311\n",
      "Epoch 98/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8277\n",
      "Epoch 98: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4283 - accuracy: 0.8277 - val_loss: 0.4187 - val_accuracy: 0.8311\n",
      "Epoch 99/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.8295\n",
      "Epoch 99: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4337 - accuracy: 0.8295 - val_loss: 0.4305 - val_accuracy: 0.8289\n",
      "Epoch 100/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.8319\n",
      "Epoch 100: val_loss did not improve from 0.41755\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4266 - accuracy: 0.8319 - val_loss: 0.4589 - val_accuracy: 0.8057\n",
      "Epoch 101/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8288\n",
      "Epoch 101: val_loss improved from 0.41755 to 0.41602, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4318 - accuracy: 0.8288 - val_loss: 0.4160 - val_accuracy: 0.8411\n",
      "Epoch 102/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8294\n",
      "Epoch 102: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4300 - accuracy: 0.8294 - val_loss: 0.4289 - val_accuracy: 0.8271\n",
      "Epoch 103/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8269\n",
      "Epoch 103: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4284 - accuracy: 0.8269 - val_loss: 0.4292 - val_accuracy: 0.8264\n",
      "Epoch 104/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8328\n",
      "Epoch 104: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4183 - accuracy: 0.8328 - val_loss: 0.4303 - val_accuracy: 0.8296\n",
      "Epoch 105/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8225\n",
      "Epoch 105: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4373 - accuracy: 0.8225 - val_loss: 0.4414 - val_accuracy: 0.8203\n",
      "Epoch 106/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8331\n",
      "Epoch 106: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4277 - accuracy: 0.8331 - val_loss: 0.4228 - val_accuracy: 0.8346\n",
      "Epoch 107/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8257\n",
      "Epoch 107: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4308 - accuracy: 0.8257 - val_loss: 0.4253 - val_accuracy: 0.8261\n",
      "Epoch 108/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8297\n",
      "Epoch 108: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4252 - accuracy: 0.8297 - val_loss: 0.4306 - val_accuracy: 0.8225\n",
      "Epoch 109/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.8285\n",
      "Epoch 109: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4279 - accuracy: 0.8285 - val_loss: 0.4233 - val_accuracy: 0.8300\n",
      "Epoch 110/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.8243\n",
      "Epoch 110: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4331 - accuracy: 0.8243 - val_loss: 0.4191 - val_accuracy: 0.8372\n",
      "Epoch 111/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8323\n",
      "Epoch 111: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4182 - accuracy: 0.8323 - val_loss: 0.4354 - val_accuracy: 0.8210\n",
      "Epoch 112/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.8314\n",
      "Epoch 112: val_loss did not improve from 0.41602\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4286 - accuracy: 0.8314 - val_loss: 0.4263 - val_accuracy: 0.8354\n",
      "Epoch 113/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8299\n",
      "Epoch 113: val_loss improved from 0.41602 to 0.41179, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4256 - accuracy: 0.8299 - val_loss: 0.4118 - val_accuracy: 0.8354\n",
      "Epoch 114/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8331\n",
      "Epoch 114: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4252 - accuracy: 0.8331 - val_loss: 0.4238 - val_accuracy: 0.8250\n",
      "Epoch 115/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.8260\n",
      "Epoch 115: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4324 - accuracy: 0.8260 - val_loss: 0.4294 - val_accuracy: 0.8243\n",
      "Epoch 116/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8292\n",
      "Epoch 116: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4291 - accuracy: 0.8292 - val_loss: 0.4252 - val_accuracy: 0.8289\n",
      "Epoch 117/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8346\n",
      "Epoch 117: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 123s 606ms/step - loss: 0.4238 - accuracy: 0.8346 - val_loss: 0.4370 - val_accuracy: 0.8182\n",
      "Epoch 118/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.8334\n",
      "Epoch 118: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 137s 671ms/step - loss: 0.4278 - accuracy: 0.8334 - val_loss: 0.4259 - val_accuracy: 0.8400\n",
      "Epoch 119/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.8337\n",
      "Epoch 119: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 135s 660ms/step - loss: 0.4222 - accuracy: 0.8337 - val_loss: 0.4167 - val_accuracy: 0.8311\n",
      "Epoch 120/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.8326\n",
      "Epoch 120: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 125s 614ms/step - loss: 0.4330 - accuracy: 0.8326 - val_loss: 0.4198 - val_accuracy: 0.8329\n",
      "Epoch 121/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8302\n",
      "Epoch 121: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 125s 613ms/step - loss: 0.4223 - accuracy: 0.8302 - val_loss: 0.4204 - val_accuracy: 0.8450\n",
      "Epoch 122/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.8326\n",
      "Epoch 122: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4267 - accuracy: 0.8326 - val_loss: 0.4588 - val_accuracy: 0.8121\n",
      "Epoch 123/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8357\n",
      "Epoch 123: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4248 - accuracy: 0.8357 - val_loss: 0.4199 - val_accuracy: 0.8368\n",
      "Epoch 124/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8329\n",
      "Epoch 124: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4223 - accuracy: 0.8329 - val_loss: 0.4407 - val_accuracy: 0.8171\n",
      "Epoch 125/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.8303\n",
      "Epoch 125: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4292 - accuracy: 0.8303 - val_loss: 0.4223 - val_accuracy: 0.8379\n",
      "Epoch 126/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8312\n",
      "Epoch 126: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4262 - accuracy: 0.8312 - val_loss: 0.4164 - val_accuracy: 0.8400\n",
      "Epoch 127/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.8322\n",
      "Epoch 127: val_loss did not improve from 0.41179\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4257 - accuracy: 0.8322 - val_loss: 0.4186 - val_accuracy: 0.8339\n",
      "Epoch 128/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.8351\n",
      "Epoch 128: val_loss improved from 0.41179 to 0.41177, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 605ms/step - loss: 0.4176 - accuracy: 0.8351 - val_loss: 0.4118 - val_accuracy: 0.8414\n",
      "Epoch 129/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.8320\n",
      "Epoch 129: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4212 - accuracy: 0.8320 - val_loss: 0.4152 - val_accuracy: 0.8379\n",
      "Epoch 130/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4253 - accuracy: 0.8365\n",
      "Epoch 130: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4253 - accuracy: 0.8365 - val_loss: 0.4371 - val_accuracy: 0.8196\n",
      "Epoch 131/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8322\n",
      "Epoch 131: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4245 - accuracy: 0.8322 - val_loss: 0.4220 - val_accuracy: 0.8311\n",
      "Epoch 132/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8325\n",
      "Epoch 132: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4178 - accuracy: 0.8325 - val_loss: 0.4350 - val_accuracy: 0.8250\n",
      "Epoch 133/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8328\n",
      "Epoch 133: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4238 - accuracy: 0.8328 - val_loss: 0.4272 - val_accuracy: 0.8250\n",
      "Epoch 134/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4251 - accuracy: 0.8315\n",
      "Epoch 134: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4251 - accuracy: 0.8315 - val_loss: 0.4337 - val_accuracy: 0.8250\n",
      "Epoch 135/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8317\n",
      "Epoch 135: val_loss did not improve from 0.41177\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4236 - accuracy: 0.8317 - val_loss: 0.4311 - val_accuracy: 0.8225\n",
      "Epoch 136/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8337\n",
      "Epoch 136: val_loss improved from 0.41177 to 0.41081, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4250 - accuracy: 0.8337 - val_loss: 0.4108 - val_accuracy: 0.8397\n",
      "Epoch 137/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8375\n",
      "Epoch 137: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4241 - accuracy: 0.8375 - val_loss: 0.4138 - val_accuracy: 0.8339\n",
      "Epoch 138/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.8357\n",
      "Epoch 138: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4148 - accuracy: 0.8357 - val_loss: 0.4279 - val_accuracy: 0.8236\n",
      "Epoch 139/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8346\n",
      "Epoch 139: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4201 - accuracy: 0.8346 - val_loss: 0.4201 - val_accuracy: 0.8336\n",
      "Epoch 140/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8342\n",
      "Epoch 140: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4213 - accuracy: 0.8342 - val_loss: 0.4187 - val_accuracy: 0.8321\n",
      "Epoch 141/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8329\n",
      "Epoch 141: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4294 - accuracy: 0.8329 - val_loss: 0.4232 - val_accuracy: 0.8350\n",
      "Epoch 142/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.8325\n",
      "Epoch 142: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4315 - accuracy: 0.8325 - val_loss: 0.4340 - val_accuracy: 0.8250\n",
      "Epoch 143/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8297\n",
      "Epoch 143: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4242 - accuracy: 0.8297 - val_loss: 0.4227 - val_accuracy: 0.8346\n",
      "Epoch 144/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8311\n",
      "Epoch 144: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4210 - accuracy: 0.8311 - val_loss: 0.4152 - val_accuracy: 0.8397\n",
      "Epoch 145/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8303\n",
      "Epoch 145: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 123s 604ms/step - loss: 0.4241 - accuracy: 0.8303 - val_loss: 0.4174 - val_accuracy: 0.8414\n",
      "Epoch 146/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8294\n",
      "Epoch 146: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4268 - accuracy: 0.8294 - val_loss: 0.4361 - val_accuracy: 0.8271\n",
      "Epoch 147/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.8326\n",
      "Epoch 147: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4185 - accuracy: 0.8326 - val_loss: 0.4480 - val_accuracy: 0.8146\n",
      "Epoch 148/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.8346\n",
      "Epoch 148: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4177 - accuracy: 0.8346 - val_loss: 0.4192 - val_accuracy: 0.8314\n",
      "Epoch 149/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4167 - accuracy: 0.8395\n",
      "Epoch 149: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 123s 605ms/step - loss: 0.4167 - accuracy: 0.8395 - val_loss: 0.4219 - val_accuracy: 0.8271\n",
      "Epoch 150/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8337\n",
      "Epoch 150: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4182 - accuracy: 0.8337 - val_loss: 0.4112 - val_accuracy: 0.8393\n",
      "Epoch 151/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8412\n",
      "Epoch 151: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4138 - accuracy: 0.8412 - val_loss: 0.4200 - val_accuracy: 0.8300\n",
      "Epoch 152/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.8372\n",
      "Epoch 152: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4146 - accuracy: 0.8372 - val_loss: 0.4347 - val_accuracy: 0.8236\n",
      "Epoch 153/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.8418\n",
      "Epoch 153: val_loss did not improve from 0.41081\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4180 - accuracy: 0.8418 - val_loss: 0.4149 - val_accuracy: 0.8397\n",
      "Epoch 154/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4152 - accuracy: 0.8354\n",
      "Epoch 154: val_loss improved from 0.41081 to 0.40794, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 604ms/step - loss: 0.4152 - accuracy: 0.8354 - val_loss: 0.4079 - val_accuracy: 0.8386\n",
      "Epoch 155/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8395\n",
      "Epoch 155: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.4119 - accuracy: 0.8395 - val_loss: 0.4213 - val_accuracy: 0.8307\n",
      "Epoch 156/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4081 - accuracy: 0.8444\n",
      "Epoch 156: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4081 - accuracy: 0.8444 - val_loss: 0.4170 - val_accuracy: 0.8411\n",
      "Epoch 157/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8358\n",
      "Epoch 157: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.4233 - accuracy: 0.8358 - val_loss: 0.4402 - val_accuracy: 0.8185\n",
      "Epoch 158/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8352\n",
      "Epoch 158: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4088 - accuracy: 0.8352 - val_loss: 0.4290 - val_accuracy: 0.8232\n",
      "Epoch 159/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.8361\n",
      "Epoch 159: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4145 - accuracy: 0.8361 - val_loss: 0.4168 - val_accuracy: 0.8386\n",
      "Epoch 160/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8391\n",
      "Epoch 160: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4162 - accuracy: 0.8391 - val_loss: 0.4251 - val_accuracy: 0.8296\n",
      "Epoch 161/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.8375\n",
      "Epoch 161: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4179 - accuracy: 0.8375 - val_loss: 0.4222 - val_accuracy: 0.8332\n",
      "Epoch 162/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8392\n",
      "Epoch 162: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4159 - accuracy: 0.8392 - val_loss: 0.4180 - val_accuracy: 0.8379\n",
      "Epoch 163/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8357\n",
      "Epoch 163: val_loss did not improve from 0.40794\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4101 - accuracy: 0.8357 - val_loss: 0.4223 - val_accuracy: 0.8282\n",
      "Epoch 164/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4105 - accuracy: 0.8366\n",
      "Epoch 164: val_loss improved from 0.40794 to 0.40700, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 125s 615ms/step - loss: 0.4105 - accuracy: 0.8366 - val_loss: 0.4070 - val_accuracy: 0.8386\n",
      "Epoch 165/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.8394\n",
      "Epoch 165: val_loss did not improve from 0.40700\n",
      "204/204 [==============================] - 119s 586ms/step - loss: 0.4066 - accuracy: 0.8394 - val_loss: 0.4221 - val_accuracy: 0.8354\n",
      "Epoch 166/200\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8384\n",
      "Epoch 166: val_loss did not improve from 0.40700\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4119 - accuracy: 0.8384 - val_loss: 0.4352 - val_accuracy: 0.8214\n",
      "Epoch 167/200\n",
      "112/204 [===============>..............] - ETA: 39s - loss: 0.4243 - accuracy: 0.8345"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josei\\OneDrive\\Documentos\\skin_cancer_detection\\scripts\\main\\mainv6.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     x_train, y_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     batch_size \u001b[39m=\u001b[39;49m BATCH_SIZE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m NUM_EPOCHS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [early_stopping, model_checkpoint],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     validation_data \u001b[39m=\u001b[39;49m (x_test, y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josei/OneDrive/Documentos/skin_cancer_detection/scripts/main/mainv6.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    callbacks = [early_stopping, model_checkpoint],\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {0: .4f}. Test accuracy: {1: .2f}%'.format(test_loss, test_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn1_MobilNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "best_model = load_model('best_cnn1_MobilNet.h5')\n",
    "\n",
    "# Hacer predicciones con el mejor modelo\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {0: .4f}. Test accuracy: {1: .2f}%'.format(test_loss, test_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Supongamos que tienes un historial llamado 'history' con registros de pérdida y precisión para entrenamiento y validación.\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Crear subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Gráfico para pérdida\n",
    "plt.sca(axes[0])\n",
    "sns.lineplot(x=epochs, y=train_loss, label='Train Loss')\n",
    "sns.lineplot(x=epochs, y=val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico para precisión\n",
    "plt.sca(axes[1])\n",
    "sns.lineplot(x=epochs, y=train_accuracy, label='Train Accuracy')\n",
    "sns.lineplot(x=epochs, y=val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Ajusta la disposición\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra la figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tienes las etiquetas reales (y_true) y las predicciones (y_pred).\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)\n",
    "\n",
    "class_labels = [\"bcc\", \"scc\"]\n",
    "\n",
    "# Crear un mapa de calor para la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Supongamos que tienes etiquetas reales (y_true) y probabilidades de predicción (y_probs).\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Crear un gráfico de la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=fpr, y=tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"D:/SkinCancerDatasets/dataset/errors/\"+tipo+\"/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for image, true_label, predicted_label in zip(x_test, y_test, y_pred):\n",
    "    if true_label != predicted_label:\n",
    "        if predicted_label == 1:\n",
    "            label_name = \"bcc\"\n",
    "        else:\n",
    "            label_name = \"scc\"\n",
    "    \n",
    "        # Crea un directorio para la etiqueta si no existe\n",
    "        label_directory = os.path.join(output_directory, label_name)\n",
    "        os.makedirs(label_directory, exist_ok=True)\n",
    "    \n",
    "        # Genera un nombre de archivo único para la imagen\n",
    "        image_name = f\"image_{len(os.listdir(label_directory))}.jpg\"\n",
    "        output_path = os.path.join(label_directory, image_name)\n",
    "    \n",
    "        # Guarda la imagen en la carpeta de la etiqueta\n",
    "        cv.imwrite(output_path, image * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400000\n",
      "4131040.407886644\n"
     ]
    }
   ],
   "source": [
    "monto_mensual = 20000\n",
    "years = 10\n",
    "interes_anual = 0.10\n",
    "\n",
    "tiempo = years * 12\n",
    "total = 0\n",
    "\n",
    "for i in range(tiempo):\n",
    "    total = total + monto_mensual\n",
    "    total = total * (1 + interes_anual/12)\n",
    "\n",
    "print(monto_mensual*tiempo)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
