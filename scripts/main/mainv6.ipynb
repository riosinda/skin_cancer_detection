{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32     # number of images to be processed in a batch\n",
    "NUM_EPOCHS = 400     # number of times the entire dataset is passed through the network\n",
    "NUM_CLASSES = 1     # number of classes in the dataset\n",
    "t = 224             # target size of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the InceptionV3 model\n",
    "base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(t, t, 3))\n",
    "\n",
    "# Freeze the pretrained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added layer to base model\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "predictions = keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we will train\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.001)      # Adam optimizer\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    staircase=True\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loos_fn = keras.losses.BinaryCrossentropy()                 # Binary crossentropy loss function\n",
    "model.compile(optimizer=optimizer, loss=loos_fn, metrics=['accuracy'])  # Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipo = \"mirror\"\n",
    "tipo = \"black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded 6518\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "dataTr = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/train/bcc/\",'*.jpg')):\n",
    "    dataTr.append([1, cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC)])\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/train/scc/\",'*.jpg')):\n",
    "    dataTr.append([0, cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC)])\n",
    "\n",
    "print('Images loaded', len(dataTr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "for i, j in dataTr:\n",
    "    x_train.append(j)\n",
    "    y_train.append(i)\n",
    "x_train = np.array(x_train)/255 # Normalize\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded 2794\n",
      "Labels loaded 2794\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/validation/bcc/\",'*.jpg')):\n",
    "    x_test.append(cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC))\n",
    "    y_test.append(1)\n",
    "for filename in glob.glob(os.path.join(\"D:/SkinCancerDatasets/dataset/\"+tipo+\"/validation/scc/\",'*.jpg')):\n",
    "    x_test.append(cv.resize(cv.imread(filename), dsize=(t, t), interpolation=cv.INTER_CUBIC))\n",
    "    y_test.append(0)\n",
    "\n",
    "print('Images loaded', len(x_test))\n",
    "print('Labels loaded', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)/255 # Normalize\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 100,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'best_cnn1_MobilNet.h5'\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 2.7581 - accuracy: 0.7284\n",
      "Epoch 1: val_loss improved from inf to 0.88717, saving model to best_cnn1_MobilNet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 137s 660ms/step - loss: 2.7581 - accuracy: 0.7284 - val_loss: 0.8872 - val_accuracy: 0.7792\n",
      "Epoch 2/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.7518\n",
      "Epoch 2: val_loss improved from 0.88717 to 0.60147, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.7402 - accuracy: 0.7518 - val_loss: 0.6015 - val_accuracy: 0.7989\n",
      "Epoch 3/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.7579\n",
      "Epoch 3: val_loss improved from 0.60147 to 0.57137, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 605ms/step - loss: 0.6136 - accuracy: 0.7579 - val_loss: 0.5714 - val_accuracy: 0.7724\n",
      "Epoch 4/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.7620\n",
      "Epoch 4: val_loss did not improve from 0.57137\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.5687 - accuracy: 0.7620 - val_loss: 0.5817 - val_accuracy: 0.7208\n",
      "Epoch 5/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7562\n",
      "Epoch 5: val_loss improved from 0.57137 to 0.53307, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.5623 - accuracy: 0.7562 - val_loss: 0.5331 - val_accuracy: 0.7795\n",
      "Epoch 6/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.7722\n",
      "Epoch 6: val_loss did not improve from 0.53307\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.5365 - accuracy: 0.7722 - val_loss: 0.5561 - val_accuracy: 0.7473\n",
      "Epoch 7/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7717\n",
      "Epoch 7: val_loss improved from 0.53307 to 0.51702, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.5347 - accuracy: 0.7717 - val_loss: 0.5170 - val_accuracy: 0.7720\n",
      "Epoch 8/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.7699\n",
      "Epoch 8: val_loss improved from 0.51702 to 0.48591, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.5293 - accuracy: 0.7699 - val_loss: 0.4859 - val_accuracy: 0.8024\n",
      "Epoch 9/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.7728\n",
      "Epoch 9: val_loss did not improve from 0.48591\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.5186 - accuracy: 0.7728 - val_loss: 0.5107 - val_accuracy: 0.7745\n",
      "Epoch 10/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.7771\n",
      "Epoch 10: val_loss improved from 0.48591 to 0.47971, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.5128 - accuracy: 0.7771 - val_loss: 0.4797 - val_accuracy: 0.7967\n",
      "Epoch 11/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.7766\n",
      "Epoch 11: val_loss did not improve from 0.47971\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.5052 - accuracy: 0.7766 - val_loss: 0.4865 - val_accuracy: 0.7856\n",
      "Epoch 12/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.7841\n",
      "Epoch 12: val_loss did not improve from 0.47971\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.5012 - accuracy: 0.7841 - val_loss: 0.4797 - val_accuracy: 0.7906\n",
      "Epoch 13/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4923 - accuracy: 0.7920\n",
      "Epoch 13: val_loss improved from 0.47971 to 0.47091, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.4923 - accuracy: 0.7920 - val_loss: 0.4709 - val_accuracy: 0.8067\n",
      "Epoch 14/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.7855\n",
      "Epoch 14: val_loss did not improve from 0.47091\n",
      "204/204 [==============================] - 123s 606ms/step - loss: 0.4896 - accuracy: 0.7855 - val_loss: 0.4813 - val_accuracy: 0.7799\n",
      "Epoch 15/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.7855\n",
      "Epoch 15: val_loss improved from 0.47091 to 0.46311, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4966 - accuracy: 0.7855 - val_loss: 0.4631 - val_accuracy: 0.8053\n",
      "Epoch 16/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.7910\n",
      "Epoch 16: val_loss did not improve from 0.46311\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4840 - accuracy: 0.7910 - val_loss: 0.4722 - val_accuracy: 0.8028\n",
      "Epoch 17/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.8002\n",
      "Epoch 17: val_loss did not improve from 0.46311\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4795 - accuracy: 0.8002 - val_loss: 0.4637 - val_accuracy: 0.8096\n",
      "Epoch 18/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.7910\n",
      "Epoch 18: val_loss improved from 0.46311 to 0.46216, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4823 - accuracy: 0.7910 - val_loss: 0.4622 - val_accuracy: 0.8074\n",
      "Epoch 19/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.7990\n",
      "Epoch 19: val_loss improved from 0.46216 to 0.45420, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4783 - accuracy: 0.7990 - val_loss: 0.4542 - val_accuracy: 0.8082\n",
      "Epoch 20/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.7955\n",
      "Epoch 20: val_loss did not improve from 0.45420\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4707 - accuracy: 0.7955 - val_loss: 0.4790 - val_accuracy: 0.7924\n",
      "Epoch 21/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.7953\n",
      "Epoch 21: val_loss did not improve from 0.45420\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4786 - accuracy: 0.7953 - val_loss: 0.4587 - val_accuracy: 0.8185\n",
      "Epoch 22/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.7959\n",
      "Epoch 22: val_loss did not improve from 0.45420\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4734 - accuracy: 0.7959 - val_loss: 0.4702 - val_accuracy: 0.7917\n",
      "Epoch 23/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.8015\n",
      "Epoch 23: val_loss did not improve from 0.45420\n",
      "204/204 [==============================] - 120s 589ms/step - loss: 0.4697 - accuracy: 0.8015 - val_loss: 0.4797 - val_accuracy: 0.7910\n",
      "Epoch 24/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.8004\n",
      "Epoch 24: val_loss improved from 0.45420 to 0.45029, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4688 - accuracy: 0.8004 - val_loss: 0.4503 - val_accuracy: 0.8264\n",
      "Epoch 25/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.7990\n",
      "Epoch 25: val_loss did not improve from 0.45029\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4666 - accuracy: 0.7990 - val_loss: 0.4676 - val_accuracy: 0.7924\n",
      "Epoch 26/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8045\n",
      "Epoch 26: val_loss improved from 0.45029 to 0.44230, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4602 - accuracy: 0.8045 - val_loss: 0.4423 - val_accuracy: 0.8236\n",
      "Epoch 27/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.8007\n",
      "Epoch 27: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4630 - accuracy: 0.8007 - val_loss: 0.4443 - val_accuracy: 0.8207\n",
      "Epoch 28/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.8036\n",
      "Epoch 28: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4614 - accuracy: 0.8036 - val_loss: 0.4433 - val_accuracy: 0.8189\n",
      "Epoch 29/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.8010\n",
      "Epoch 29: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 123s 605ms/step - loss: 0.4666 - accuracy: 0.8010 - val_loss: 0.4459 - val_accuracy: 0.8196\n",
      "Epoch 30/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.8006\n",
      "Epoch 30: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 122s 596ms/step - loss: 0.4659 - accuracy: 0.8006 - val_loss: 0.4507 - val_accuracy: 0.8182\n",
      "Epoch 31/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4653 - accuracy: 0.8022\n",
      "Epoch 31: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4653 - accuracy: 0.8022 - val_loss: 0.4506 - val_accuracy: 0.8099\n",
      "Epoch 32/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.7999\n",
      "Epoch 32: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4660 - accuracy: 0.7999 - val_loss: 0.4456 - val_accuracy: 0.8157\n",
      "Epoch 33/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.8007\n",
      "Epoch 33: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4666 - accuracy: 0.8007 - val_loss: 0.4435 - val_accuracy: 0.8164\n",
      "Epoch 34/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8071\n",
      "Epoch 34: val_loss did not improve from 0.44230\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4579 - accuracy: 0.8071 - val_loss: 0.4536 - val_accuracy: 0.8160\n",
      "Epoch 35/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.8128\n",
      "Epoch 35: val_loss improved from 0.44230 to 0.43686, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4616 - accuracy: 0.8128 - val_loss: 0.4369 - val_accuracy: 0.8300\n",
      "Epoch 36/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.8128\n",
      "Epoch 36: val_loss did not improve from 0.43686\n",
      "204/204 [==============================] - 124s 608ms/step - loss: 0.4501 - accuracy: 0.8128 - val_loss: 0.4400 - val_accuracy: 0.8132\n",
      "Epoch 37/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8116\n",
      "Epoch 37: val_loss improved from 0.43686 to 0.43515, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4482 - accuracy: 0.8116 - val_loss: 0.4352 - val_accuracy: 0.8210\n",
      "Epoch 38/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8052\n",
      "Epoch 38: val_loss did not improve from 0.43515\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4550 - accuracy: 0.8052 - val_loss: 0.4373 - val_accuracy: 0.8185\n",
      "Epoch 39/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.8068\n",
      "Epoch 39: val_loss improved from 0.43515 to 0.43333, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4545 - accuracy: 0.8068 - val_loss: 0.4333 - val_accuracy: 0.8261\n",
      "Epoch 40/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4555 - accuracy: 0.8059\n",
      "Epoch 40: val_loss did not improve from 0.43333\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4555 - accuracy: 0.8059 - val_loss: 0.4414 - val_accuracy: 0.8146\n",
      "Epoch 41/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.8059\n",
      "Epoch 41: val_loss did not improve from 0.43333\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4539 - accuracy: 0.8059 - val_loss: 0.4365 - val_accuracy: 0.8207\n",
      "Epoch 42/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8136\n",
      "Epoch 42: val_loss did not improve from 0.43333\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4489 - accuracy: 0.8136 - val_loss: 0.4443 - val_accuracy: 0.8200\n",
      "Epoch 43/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.8085\n",
      "Epoch 43: val_loss did not improve from 0.43333\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4529 - accuracy: 0.8085 - val_loss: 0.4715 - val_accuracy: 0.7981\n",
      "Epoch 44/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8085\n",
      "Epoch 44: val_loss did not improve from 0.43333\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4482 - accuracy: 0.8085 - val_loss: 0.4343 - val_accuracy: 0.8257\n",
      "Epoch 45/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8118\n",
      "Epoch 45: val_loss improved from 0.43333 to 0.43103, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4513 - accuracy: 0.8118 - val_loss: 0.4310 - val_accuracy: 0.8246\n",
      "Epoch 46/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8131\n",
      "Epoch 46: val_loss improved from 0.43103 to 0.43025, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4474 - accuracy: 0.8131 - val_loss: 0.4302 - val_accuracy: 0.8275\n",
      "Epoch 47/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.8183\n",
      "Epoch 47: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4463 - accuracy: 0.8183 - val_loss: 0.4406 - val_accuracy: 0.8278\n",
      "Epoch 48/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.8147\n",
      "Epoch 48: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4484 - accuracy: 0.8147 - val_loss: 0.4318 - val_accuracy: 0.8218\n",
      "Epoch 49/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8185\n",
      "Epoch 49: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4453 - accuracy: 0.8185 - val_loss: 0.4468 - val_accuracy: 0.8092\n",
      "Epoch 50/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.8211\n",
      "Epoch 50: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4422 - accuracy: 0.8211 - val_loss: 0.4525 - val_accuracy: 0.8089\n",
      "Epoch 51/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8210\n",
      "Epoch 51: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4415 - accuracy: 0.8210 - val_loss: 0.4353 - val_accuracy: 0.8275\n",
      "Epoch 52/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.8167\n",
      "Epoch 52: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4419 - accuracy: 0.8167 - val_loss: 0.4381 - val_accuracy: 0.8232\n",
      "Epoch 53/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.8196\n",
      "Epoch 53: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4423 - accuracy: 0.8196 - val_loss: 0.4303 - val_accuracy: 0.8200\n",
      "Epoch 54/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8202\n",
      "Epoch 54: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.4391 - accuracy: 0.8202 - val_loss: 0.4328 - val_accuracy: 0.8196\n",
      "Epoch 55/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8249\n",
      "Epoch 55: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4318 - accuracy: 0.8249 - val_loss: 0.4423 - val_accuracy: 0.8153\n",
      "Epoch 56/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8260\n",
      "Epoch 56: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.4347 - accuracy: 0.8260 - val_loss: 0.4326 - val_accuracy: 0.8282\n",
      "Epoch 57/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8194\n",
      "Epoch 57: val_loss did not improve from 0.43025\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4380 - accuracy: 0.8194 - val_loss: 0.4310 - val_accuracy: 0.8218\n",
      "Epoch 58/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8207\n",
      "Epoch 58: val_loss improved from 0.43025 to 0.42574, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4392 - accuracy: 0.8207 - val_loss: 0.4257 - val_accuracy: 0.8314\n",
      "Epoch 59/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8269\n",
      "Epoch 59: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4373 - accuracy: 0.8269 - val_loss: 0.4326 - val_accuracy: 0.8268\n",
      "Epoch 60/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4378 - accuracy: 0.8214\n",
      "Epoch 60: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4378 - accuracy: 0.8214 - val_loss: 0.4292 - val_accuracy: 0.8314\n",
      "Epoch 61/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.8191\n",
      "Epoch 61: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4452 - accuracy: 0.8191 - val_loss: 0.4293 - val_accuracy: 0.8271\n",
      "Epoch 62/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8180\n",
      "Epoch 62: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4415 - accuracy: 0.8180 - val_loss: 0.4340 - val_accuracy: 0.8218\n",
      "Epoch 63/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8231\n",
      "Epoch 63: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4345 - accuracy: 0.8231 - val_loss: 0.4351 - val_accuracy: 0.8214\n",
      "Epoch 64/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.8210\n",
      "Epoch 64: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4381 - accuracy: 0.8210 - val_loss: 0.4323 - val_accuracy: 0.8278\n",
      "Epoch 65/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.8222\n",
      "Epoch 65: val_loss did not improve from 0.42574\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4352 - accuracy: 0.8222 - val_loss: 0.4265 - val_accuracy: 0.8329\n",
      "Epoch 66/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.8217\n",
      "Epoch 66: val_loss improved from 0.42574 to 0.42346, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4405 - accuracy: 0.8217 - val_loss: 0.4235 - val_accuracy: 0.8350\n",
      "Epoch 67/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.8216\n",
      "Epoch 67: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4406 - accuracy: 0.8216 - val_loss: 0.4457 - val_accuracy: 0.8125\n",
      "Epoch 68/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8200\n",
      "Epoch 68: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4367 - accuracy: 0.8200 - val_loss: 0.4289 - val_accuracy: 0.8214\n",
      "Epoch 69/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.8217\n",
      "Epoch 69: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4346 - accuracy: 0.8217 - val_loss: 0.4330 - val_accuracy: 0.8193\n",
      "Epoch 70/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8280\n",
      "Epoch 70: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4296 - accuracy: 0.8280 - val_loss: 0.4328 - val_accuracy: 0.8286\n",
      "Epoch 71/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.8246\n",
      "Epoch 71: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4304 - accuracy: 0.8246 - val_loss: 0.4286 - val_accuracy: 0.8271\n",
      "Epoch 72/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.8193\n",
      "Epoch 72: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 120s 588ms/step - loss: 0.4344 - accuracy: 0.8193 - val_loss: 0.4298 - val_accuracy: 0.8210\n",
      "Epoch 73/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.8282\n",
      "Epoch 73: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4286 - accuracy: 0.8282 - val_loss: 0.4260 - val_accuracy: 0.8282\n",
      "Epoch 74/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8242\n",
      "Epoch 74: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4336 - accuracy: 0.8242 - val_loss: 0.4310 - val_accuracy: 0.8250\n",
      "Epoch 75/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.8309\n",
      "Epoch 75: val_loss did not improve from 0.42346\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4257 - accuracy: 0.8309 - val_loss: 0.4251 - val_accuracy: 0.8289\n",
      "Epoch 76/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8193\n",
      "Epoch 76: val_loss improved from 0.42346 to 0.42204, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4407 - accuracy: 0.8193 - val_loss: 0.4220 - val_accuracy: 0.8329\n",
      "Epoch 77/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8196\n",
      "Epoch 77: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4339 - accuracy: 0.8196 - val_loss: 0.4259 - val_accuracy: 0.8318\n",
      "Epoch 78/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8274\n",
      "Epoch 78: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4310 - accuracy: 0.8274 - val_loss: 0.4274 - val_accuracy: 0.8346\n",
      "Epoch 79/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8314\n",
      "Epoch 79: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4273 - accuracy: 0.8314 - val_loss: 0.4313 - val_accuracy: 0.8189\n",
      "Epoch 80/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8260\n",
      "Epoch 80: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4316 - accuracy: 0.8260 - val_loss: 0.4285 - val_accuracy: 0.8278\n",
      "Epoch 81/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8248\n",
      "Epoch 81: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4263 - accuracy: 0.8248 - val_loss: 0.4343 - val_accuracy: 0.8293\n",
      "Epoch 82/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8246\n",
      "Epoch 82: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4361 - accuracy: 0.8246 - val_loss: 0.4368 - val_accuracy: 0.8185\n",
      "Epoch 83/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.8328\n",
      "Epoch 83: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4222 - accuracy: 0.8328 - val_loss: 0.4389 - val_accuracy: 0.8178\n",
      "Epoch 84/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.8245\n",
      "Epoch 84: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4338 - accuracy: 0.8245 - val_loss: 0.4305 - val_accuracy: 0.8239\n",
      "Epoch 85/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8257\n",
      "Epoch 85: val_loss did not improve from 0.42204\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4317 - accuracy: 0.8257 - val_loss: 0.4355 - val_accuracy: 0.8243\n",
      "Epoch 86/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8202\n",
      "Epoch 86: val_loss improved from 0.42204 to 0.41313, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 120s 588ms/step - loss: 0.4316 - accuracy: 0.8202 - val_loss: 0.4131 - val_accuracy: 0.8400\n",
      "Epoch 87/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.8246\n",
      "Epoch 87: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4313 - accuracy: 0.8246 - val_loss: 0.4293 - val_accuracy: 0.8228\n",
      "Epoch 88/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4251 - accuracy: 0.8279\n",
      "Epoch 88: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4251 - accuracy: 0.8279 - val_loss: 0.4150 - val_accuracy: 0.8418\n",
      "Epoch 89/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8276\n",
      "Epoch 89: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 128s 626ms/step - loss: 0.4289 - accuracy: 0.8276 - val_loss: 0.4463 - val_accuracy: 0.8039\n",
      "Epoch 90/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.8254\n",
      "Epoch 90: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 128s 627ms/step - loss: 0.4379 - accuracy: 0.8254 - val_loss: 0.4219 - val_accuracy: 0.8271\n",
      "Epoch 91/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.8214\n",
      "Epoch 91: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4377 - accuracy: 0.8214 - val_loss: 0.4338 - val_accuracy: 0.8236\n",
      "Epoch 92/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8299\n",
      "Epoch 92: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4308 - accuracy: 0.8299 - val_loss: 0.4265 - val_accuracy: 0.8239\n",
      "Epoch 93/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8239\n",
      "Epoch 93: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4293 - accuracy: 0.8239 - val_loss: 0.4323 - val_accuracy: 0.8243\n",
      "Epoch 94/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8299\n",
      "Epoch 94: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4300 - accuracy: 0.8299 - val_loss: 0.4329 - val_accuracy: 0.8289\n",
      "Epoch 95/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8208\n",
      "Epoch 95: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4340 - accuracy: 0.8208 - val_loss: 0.4363 - val_accuracy: 0.8210\n",
      "Epoch 96/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.8253\n",
      "Epoch 96: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 621ms/step - loss: 0.4280 - accuracy: 0.8253 - val_loss: 0.4213 - val_accuracy: 0.8304\n",
      "Epoch 97/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8348\n",
      "Epoch 97: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 625ms/step - loss: 0.4247 - accuracy: 0.8348 - val_loss: 0.4200 - val_accuracy: 0.8321\n",
      "Epoch 98/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8262\n",
      "Epoch 98: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 625ms/step - loss: 0.4302 - accuracy: 0.8262 - val_loss: 0.4255 - val_accuracy: 0.8343\n",
      "Epoch 99/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.8233\n",
      "Epoch 99: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 128s 627ms/step - loss: 0.4346 - accuracy: 0.8233 - val_loss: 0.4304 - val_accuracy: 0.8218\n",
      "Epoch 100/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4239 - accuracy: 0.8271\n",
      "Epoch 100: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.4239 - accuracy: 0.8271 - val_loss: 0.4195 - val_accuracy: 0.8364\n",
      "Epoch 101/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8253\n",
      "Epoch 101: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 621ms/step - loss: 0.4298 - accuracy: 0.8253 - val_loss: 0.4150 - val_accuracy: 0.8350\n",
      "Epoch 102/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.8305\n",
      "Epoch 102: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 128s 629ms/step - loss: 0.4272 - accuracy: 0.8305 - val_loss: 0.4167 - val_accuracy: 0.8382\n",
      "Epoch 103/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8342\n",
      "Epoch 103: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 622ms/step - loss: 0.4207 - accuracy: 0.8342 - val_loss: 0.4312 - val_accuracy: 0.8293\n",
      "Epoch 104/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.8237\n",
      "Epoch 104: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 126s 620ms/step - loss: 0.4288 - accuracy: 0.8237 - val_loss: 0.4318 - val_accuracy: 0.8296\n",
      "Epoch 105/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8289\n",
      "Epoch 105: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 126s 618ms/step - loss: 0.4255 - accuracy: 0.8289 - val_loss: 0.4260 - val_accuracy: 0.8250\n",
      "Epoch 106/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8314\n",
      "Epoch 106: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 127s 624ms/step - loss: 0.4249 - accuracy: 0.8314 - val_loss: 0.4308 - val_accuracy: 0.8257\n",
      "Epoch 107/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8200\n",
      "Epoch 107: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 129s 634ms/step - loss: 0.4321 - accuracy: 0.8200 - val_loss: 0.4371 - val_accuracy: 0.8282\n",
      "Epoch 108/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8311\n",
      "Epoch 108: val_loss did not improve from 0.41313\n",
      "204/204 [==============================] - 129s 633ms/step - loss: 0.4183 - accuracy: 0.8311 - val_loss: 0.4168 - val_accuracy: 0.8336\n",
      "Epoch 109/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8285\n",
      "Epoch 109: val_loss improved from 0.41313 to 0.41163, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 129s 632ms/step - loss: 0.4214 - accuracy: 0.8285 - val_loss: 0.4116 - val_accuracy: 0.8346\n",
      "Epoch 110/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8365\n",
      "Epoch 110: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 128s 629ms/step - loss: 0.4164 - accuracy: 0.8365 - val_loss: 0.4174 - val_accuracy: 0.8307\n",
      "Epoch 111/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8236\n",
      "Epoch 111: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 128s 629ms/step - loss: 0.4326 - accuracy: 0.8236 - val_loss: 0.4280 - val_accuracy: 0.8203\n",
      "Epoch 112/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8303\n",
      "Epoch 112: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 129s 633ms/step - loss: 0.4256 - accuracy: 0.8303 - val_loss: 0.4277 - val_accuracy: 0.8236\n",
      "Epoch 113/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8351\n",
      "Epoch 113: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 128s 627ms/step - loss: 0.4197 - accuracy: 0.8351 - val_loss: 0.4321 - val_accuracy: 0.8210\n",
      "Epoch 114/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8312\n",
      "Epoch 114: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4245 - accuracy: 0.8312 - val_loss: 0.4276 - val_accuracy: 0.8282\n",
      "Epoch 115/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.8338\n",
      "Epoch 115: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4163 - accuracy: 0.8338 - val_loss: 0.4151 - val_accuracy: 0.8318\n",
      "Epoch 116/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8262\n",
      "Epoch 116: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4236 - accuracy: 0.8262 - val_loss: 0.4231 - val_accuracy: 0.8336\n",
      "Epoch 117/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8269\n",
      "Epoch 117: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4223 - accuracy: 0.8269 - val_loss: 0.4147 - val_accuracy: 0.8257\n",
      "Epoch 118/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4174 - accuracy: 0.8338\n",
      "Epoch 118: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.4174 - accuracy: 0.8338 - val_loss: 0.4198 - val_accuracy: 0.8300\n",
      "Epoch 119/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8251\n",
      "Epoch 119: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4233 - accuracy: 0.8251 - val_loss: 0.4198 - val_accuracy: 0.8236\n",
      "Epoch 120/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8322\n",
      "Epoch 120: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4198 - accuracy: 0.8322 - val_loss: 0.4200 - val_accuracy: 0.8275\n",
      "Epoch 121/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.8331\n",
      "Epoch 121: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4205 - accuracy: 0.8331 - val_loss: 0.4278 - val_accuracy: 0.8243\n",
      "Epoch 122/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8257\n",
      "Epoch 122: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4281 - accuracy: 0.8257 - val_loss: 0.4159 - val_accuracy: 0.8321\n",
      "Epoch 123/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8248\n",
      "Epoch 123: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4284 - accuracy: 0.8248 - val_loss: 0.4471 - val_accuracy: 0.8064\n",
      "Epoch 124/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8272\n",
      "Epoch 124: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4291 - accuracy: 0.8272 - val_loss: 0.4549 - val_accuracy: 0.8021\n",
      "Epoch 125/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.8271\n",
      "Epoch 125: val_loss did not improve from 0.41163\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4230 - accuracy: 0.8271 - val_loss: 0.4225 - val_accuracy: 0.8296\n",
      "Epoch 126/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8276\n",
      "Epoch 126: val_loss improved from 0.41163 to 0.40899, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4291 - accuracy: 0.8276 - val_loss: 0.4090 - val_accuracy: 0.8404\n",
      "Epoch 127/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8346\n",
      "Epoch 127: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4189 - accuracy: 0.8346 - val_loss: 0.4267 - val_accuracy: 0.8304\n",
      "Epoch 128/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8311\n",
      "Epoch 128: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4232 - accuracy: 0.8311 - val_loss: 0.4228 - val_accuracy: 0.8336\n",
      "Epoch 129/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8302\n",
      "Epoch 129: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4202 - accuracy: 0.8302 - val_loss: 0.4211 - val_accuracy: 0.8321\n",
      "Epoch 130/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8334\n",
      "Epoch 130: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4247 - accuracy: 0.8334 - val_loss: 0.4389 - val_accuracy: 0.8182\n",
      "Epoch 131/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.8268\n",
      "Epoch 131: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4314 - accuracy: 0.8268 - val_loss: 0.4283 - val_accuracy: 0.8286\n",
      "Epoch 132/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.8306\n",
      "Epoch 132: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4180 - accuracy: 0.8306 - val_loss: 0.4395 - val_accuracy: 0.8164\n",
      "Epoch 133/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8355\n",
      "Epoch 133: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4204 - accuracy: 0.8355 - val_loss: 0.4230 - val_accuracy: 0.8296\n",
      "Epoch 134/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8337\n",
      "Epoch 134: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4191 - accuracy: 0.8337 - val_loss: 0.4164 - val_accuracy: 0.8325\n",
      "Epoch 135/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8322\n",
      "Epoch 135: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4238 - accuracy: 0.8322 - val_loss: 0.4172 - val_accuracy: 0.8307\n",
      "Epoch 136/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8295\n",
      "Epoch 136: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4252 - accuracy: 0.8295 - val_loss: 0.4229 - val_accuracy: 0.8379\n",
      "Epoch 137/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8322\n",
      "Epoch 137: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4186 - accuracy: 0.8322 - val_loss: 0.4171 - val_accuracy: 0.8300\n",
      "Epoch 138/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8300\n",
      "Epoch 138: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4199 - accuracy: 0.8300 - val_loss: 0.4466 - val_accuracy: 0.8078\n",
      "Epoch 139/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8312\n",
      "Epoch 139: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4189 - accuracy: 0.8312 - val_loss: 0.4274 - val_accuracy: 0.8236\n",
      "Epoch 140/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8337\n",
      "Epoch 140: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4204 - accuracy: 0.8337 - val_loss: 0.4223 - val_accuracy: 0.8314\n",
      "Epoch 141/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8328\n",
      "Epoch 141: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 128s 630ms/step - loss: 0.4188 - accuracy: 0.8328 - val_loss: 0.4197 - val_accuracy: 0.8296\n",
      "Epoch 142/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8283\n",
      "Epoch 142: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 123s 601ms/step - loss: 0.4255 - accuracy: 0.8283 - val_loss: 0.4193 - val_accuracy: 0.8332\n",
      "Epoch 143/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.8312\n",
      "Epoch 143: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4211 - accuracy: 0.8312 - val_loss: 0.4262 - val_accuracy: 0.8232\n",
      "Epoch 144/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8319\n",
      "Epoch 144: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4213 - accuracy: 0.8319 - val_loss: 0.4161 - val_accuracy: 0.8386\n",
      "Epoch 145/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4165 - accuracy: 0.8315\n",
      "Epoch 145: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4165 - accuracy: 0.8315 - val_loss: 0.4184 - val_accuracy: 0.8332\n",
      "Epoch 146/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8292\n",
      "Epoch 146: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4202 - accuracy: 0.8292 - val_loss: 0.4262 - val_accuracy: 0.8250\n",
      "Epoch 147/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8309\n",
      "Epoch 147: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 124s 607ms/step - loss: 0.4175 - accuracy: 0.8309 - val_loss: 0.4270 - val_accuracy: 0.8268\n",
      "Epoch 148/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.8368\n",
      "Epoch 148: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4157 - accuracy: 0.8368 - val_loss: 0.4259 - val_accuracy: 0.8296\n",
      "Epoch 149/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.8303\n",
      "Epoch 149: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 182s 894ms/step - loss: 0.4208 - accuracy: 0.8303 - val_loss: 0.4137 - val_accuracy: 0.8304\n",
      "Epoch 150/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8345 \n",
      "Epoch 150: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 11012s 54s/step - loss: 0.4158 - accuracy: 0.8345 - val_loss: 0.4141 - val_accuracy: 0.8332\n",
      "Epoch 151/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.8335\n",
      "Epoch 151: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 111s 547ms/step - loss: 0.4151 - accuracy: 0.8335 - val_loss: 0.4139 - val_accuracy: 0.8346\n",
      "Epoch 152/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.8412\n",
      "Epoch 152: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 111s 545ms/step - loss: 0.4065 - accuracy: 0.8412 - val_loss: 0.4141 - val_accuracy: 0.8368\n",
      "Epoch 153/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8378\n",
      "Epoch 153: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 110s 540ms/step - loss: 0.4144 - accuracy: 0.8378 - val_loss: 0.4365 - val_accuracy: 0.8189\n",
      "Epoch 154/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8343\n",
      "Epoch 154: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 112s 548ms/step - loss: 0.4154 - accuracy: 0.8343 - val_loss: 0.4160 - val_accuracy: 0.8304\n",
      "Epoch 155/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8374\n",
      "Epoch 155: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 111s 543ms/step - loss: 0.4131 - accuracy: 0.8374 - val_loss: 0.4101 - val_accuracy: 0.8307\n",
      "Epoch 156/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8312\n",
      "Epoch 156: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 111s 543ms/step - loss: 0.4143 - accuracy: 0.8312 - val_loss: 0.4121 - val_accuracy: 0.8275\n",
      "Epoch 157/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8377\n",
      "Epoch 157: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 110s 542ms/step - loss: 0.4080 - accuracy: 0.8377 - val_loss: 0.4100 - val_accuracy: 0.8343\n",
      "Epoch 158/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4116 - accuracy: 0.8374\n",
      "Epoch 158: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 115s 564ms/step - loss: 0.4116 - accuracy: 0.8374 - val_loss: 0.4179 - val_accuracy: 0.8264\n",
      "Epoch 159/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8361\n",
      "Epoch 159: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 111s 546ms/step - loss: 0.4072 - accuracy: 0.8361 - val_loss: 0.4141 - val_accuracy: 0.8296\n",
      "Epoch 160/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4147 - accuracy: 0.8361\n",
      "Epoch 160: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 112s 549ms/step - loss: 0.4147 - accuracy: 0.8361 - val_loss: 0.4136 - val_accuracy: 0.8304\n",
      "Epoch 161/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.8345\n",
      "Epoch 161: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 112s 551ms/step - loss: 0.4176 - accuracy: 0.8345 - val_loss: 0.4105 - val_accuracy: 0.8325\n",
      "Epoch 162/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.8363\n",
      "Epoch 162: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 117s 573ms/step - loss: 0.4170 - accuracy: 0.8363 - val_loss: 0.4180 - val_accuracy: 0.8236\n",
      "Epoch 163/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4096 - accuracy: 0.8395\n",
      "Epoch 163: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4096 - accuracy: 0.8395 - val_loss: 0.4315 - val_accuracy: 0.8099\n",
      "Epoch 164/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8325\n",
      "Epoch 164: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 131s 643ms/step - loss: 0.4102 - accuracy: 0.8325 - val_loss: 0.4352 - val_accuracy: 0.8193\n",
      "Epoch 165/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8309\n",
      "Epoch 165: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 130s 639ms/step - loss: 0.4198 - accuracy: 0.8309 - val_loss: 0.4357 - val_accuracy: 0.8121\n",
      "Epoch 166/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8309\n",
      "Epoch 166: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4133 - accuracy: 0.8309 - val_loss: 0.4160 - val_accuracy: 0.8346\n",
      "Epoch 167/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8375\n",
      "Epoch 167: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4095 - accuracy: 0.8375 - val_loss: 0.4167 - val_accuracy: 0.8332\n",
      "Epoch 168/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8325\n",
      "Epoch 168: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 124s 608ms/step - loss: 0.4150 - accuracy: 0.8325 - val_loss: 0.4288 - val_accuracy: 0.8236\n",
      "Epoch 169/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.8360\n",
      "Epoch 169: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 126s 619ms/step - loss: 0.4111 - accuracy: 0.8360 - val_loss: 0.4150 - val_accuracy: 0.8307\n",
      "Epoch 170/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8338\n",
      "Epoch 170: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 129s 633ms/step - loss: 0.4178 - accuracy: 0.8338 - val_loss: 0.4222 - val_accuracy: 0.8253\n",
      "Epoch 171/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4137 - accuracy: 0.8375\n",
      "Epoch 171: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 126s 619ms/step - loss: 0.4137 - accuracy: 0.8375 - val_loss: 0.4249 - val_accuracy: 0.8250\n",
      "Epoch 172/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.8320\n",
      "Epoch 172: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 128s 626ms/step - loss: 0.4155 - accuracy: 0.8320 - val_loss: 0.4249 - val_accuracy: 0.8114\n",
      "Epoch 173/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4114 - accuracy: 0.8329\n",
      "Epoch 173: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.4114 - accuracy: 0.8329 - val_loss: 0.4233 - val_accuracy: 0.8203\n",
      "Epoch 174/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4140 - accuracy: 0.8357\n",
      "Epoch 174: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 124s 607ms/step - loss: 0.4140 - accuracy: 0.8357 - val_loss: 0.4146 - val_accuracy: 0.8314\n",
      "Epoch 175/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8268\n",
      "Epoch 175: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4268 - accuracy: 0.8268 - val_loss: 0.4314 - val_accuracy: 0.8239\n",
      "Epoch 176/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.8329\n",
      "Epoch 176: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4193 - accuracy: 0.8329 - val_loss: 0.4149 - val_accuracy: 0.8311\n",
      "Epoch 177/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4122 - accuracy: 0.8326\n",
      "Epoch 177: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4122 - accuracy: 0.8326 - val_loss: 0.4090 - val_accuracy: 0.8321\n",
      "Epoch 178/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8340\n",
      "Epoch 178: val_loss did not improve from 0.40899\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4171 - accuracy: 0.8340 - val_loss: 0.4182 - val_accuracy: 0.8232\n",
      "Epoch 179/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8342\n",
      "Epoch 179: val_loss improved from 0.40899 to 0.40578, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4182 - accuracy: 0.8342 - val_loss: 0.4058 - val_accuracy: 0.8368\n",
      "Epoch 180/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.8378\n",
      "Epoch 180: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4110 - accuracy: 0.8378 - val_loss: 0.4164 - val_accuracy: 0.8318\n",
      "Epoch 181/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8302\n",
      "Epoch 181: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4129 - accuracy: 0.8302 - val_loss: 0.4090 - val_accuracy: 0.8375\n",
      "Epoch 182/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8348\n",
      "Epoch 182: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.4107 - accuracy: 0.8348 - val_loss: 0.4167 - val_accuracy: 0.8357\n",
      "Epoch 183/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8366\n",
      "Epoch 183: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4117 - accuracy: 0.8366 - val_loss: 0.4219 - val_accuracy: 0.8379\n",
      "Epoch 184/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.8378\n",
      "Epoch 184: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4074 - accuracy: 0.8378 - val_loss: 0.4115 - val_accuracy: 0.8357\n",
      "Epoch 185/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.8337\n",
      "Epoch 185: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4121 - accuracy: 0.8337 - val_loss: 0.4204 - val_accuracy: 0.8293\n",
      "Epoch 186/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8374\n",
      "Epoch 186: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4085 - accuracy: 0.8374 - val_loss: 0.4185 - val_accuracy: 0.8339\n",
      "Epoch 187/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8424\n",
      "Epoch 187: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4045 - accuracy: 0.8424 - val_loss: 0.4118 - val_accuracy: 0.8386\n",
      "Epoch 188/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.8338\n",
      "Epoch 188: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4177 - accuracy: 0.8338 - val_loss: 0.4070 - val_accuracy: 0.8432\n",
      "Epoch 189/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8335\n",
      "Epoch 189: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4125 - accuracy: 0.8335 - val_loss: 0.4095 - val_accuracy: 0.8354\n",
      "Epoch 190/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.8299\n",
      "Epoch 190: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4184 - accuracy: 0.8299 - val_loss: 0.4134 - val_accuracy: 0.8386\n",
      "Epoch 191/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4168 - accuracy: 0.8279\n",
      "Epoch 191: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4168 - accuracy: 0.8279 - val_loss: 0.4149 - val_accuracy: 0.8329\n",
      "Epoch 192/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8282\n",
      "Epoch 192: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 601ms/step - loss: 0.4189 - accuracy: 0.8282 - val_loss: 0.4135 - val_accuracy: 0.8343\n",
      "Epoch 193/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8328\n",
      "Epoch 193: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4143 - accuracy: 0.8328 - val_loss: 0.4264 - val_accuracy: 0.8232\n",
      "Epoch 194/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8383\n",
      "Epoch 194: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 596ms/step - loss: 0.4083 - accuracy: 0.8383 - val_loss: 0.4187 - val_accuracy: 0.8271\n",
      "Epoch 195/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8461\n",
      "Epoch 195: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4084 - accuracy: 0.8461 - val_loss: 0.4162 - val_accuracy: 0.8339\n",
      "Epoch 196/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8323\n",
      "Epoch 196: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4159 - accuracy: 0.8323 - val_loss: 0.4215 - val_accuracy: 0.8393\n",
      "Epoch 197/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.8384\n",
      "Epoch 197: val_loss did not improve from 0.40578\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4121 - accuracy: 0.8384 - val_loss: 0.4160 - val_accuracy: 0.8350\n",
      "Epoch 198/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8363\n",
      "Epoch 198: val_loss improved from 0.40578 to 0.40347, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4131 - accuracy: 0.8363 - val_loss: 0.4035 - val_accuracy: 0.8314\n",
      "Epoch 199/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8394\n",
      "Epoch 199: val_loss did not improve from 0.40347\n",
      "204/204 [==============================] - 127s 623ms/step - loss: 0.4055 - accuracy: 0.8394 - val_loss: 0.4145 - val_accuracy: 0.8325\n",
      "Epoch 200/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.8392\n",
      "Epoch 200: val_loss did not improve from 0.40347\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.4038 - accuracy: 0.8392 - val_loss: 0.4037 - val_accuracy: 0.8389\n",
      "Epoch 201/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8388\n",
      "Epoch 201: val_loss did not improve from 0.40347\n",
      "204/204 [==============================] - 123s 602ms/step - loss: 0.4097 - accuracy: 0.8388 - val_loss: 0.4037 - val_accuracy: 0.8350\n",
      "Epoch 202/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8337\n",
      "Epoch 202: val_loss improved from 0.40347 to 0.39898, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 125s 611ms/step - loss: 0.4029 - accuracy: 0.8337 - val_loss: 0.3990 - val_accuracy: 0.8368\n",
      "Epoch 203/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8404\n",
      "Epoch 203: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 123s 605ms/step - loss: 0.4030 - accuracy: 0.8404 - val_loss: 0.4212 - val_accuracy: 0.8325\n",
      "Epoch 204/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8384\n",
      "Epoch 204: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.4024 - accuracy: 0.8384 - val_loss: 0.4175 - val_accuracy: 0.8286\n",
      "Epoch 205/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8374\n",
      "Epoch 205: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4060 - accuracy: 0.8374 - val_loss: 0.4110 - val_accuracy: 0.8307\n",
      "Epoch 206/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8380\n",
      "Epoch 206: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 596ms/step - loss: 0.4083 - accuracy: 0.8380 - val_loss: 0.4214 - val_accuracy: 0.8321\n",
      "Epoch 207/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8391\n",
      "Epoch 207: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.3998 - accuracy: 0.8391 - val_loss: 0.4011 - val_accuracy: 0.8407\n",
      "Epoch 208/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4132 - accuracy: 0.8328\n",
      "Epoch 208: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.4132 - accuracy: 0.8328 - val_loss: 0.4499 - val_accuracy: 0.8150\n",
      "Epoch 209/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8449\n",
      "Epoch 209: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.3998 - accuracy: 0.8449 - val_loss: 0.4141 - val_accuracy: 0.8264\n",
      "Epoch 210/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.8351\n",
      "Epoch 210: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 119s 584ms/step - loss: 0.4108 - accuracy: 0.8351 - val_loss: 0.4153 - val_accuracy: 0.8253\n",
      "Epoch 211/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.8342\n",
      "Epoch 211: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4135 - accuracy: 0.8342 - val_loss: 0.4140 - val_accuracy: 0.8343\n",
      "Epoch 212/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8431\n",
      "Epoch 212: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4029 - accuracy: 0.8431 - val_loss: 0.4252 - val_accuracy: 0.8214\n",
      "Epoch 213/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.8469\n",
      "Epoch 213: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 594ms/step - loss: 0.4019 - accuracy: 0.8469 - val_loss: 0.4130 - val_accuracy: 0.8364\n",
      "Epoch 214/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.8404\n",
      "Epoch 214: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4062 - accuracy: 0.8404 - val_loss: 0.4071 - val_accuracy: 0.8329\n",
      "Epoch 215/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.8377\n",
      "Epoch 215: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 599ms/step - loss: 0.4064 - accuracy: 0.8377 - val_loss: 0.4207 - val_accuracy: 0.8314\n",
      "Epoch 216/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.8398\n",
      "Epoch 216: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4066 - accuracy: 0.8398 - val_loss: 0.4121 - val_accuracy: 0.8425\n",
      "Epoch 217/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8395\n",
      "Epoch 217: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 600ms/step - loss: 0.4012 - accuracy: 0.8395 - val_loss: 0.4078 - val_accuracy: 0.8375\n",
      "Epoch 218/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4137 - accuracy: 0.8295\n",
      "Epoch 218: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4137 - accuracy: 0.8295 - val_loss: 0.4098 - val_accuracy: 0.8343\n",
      "Epoch 219/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8355\n",
      "Epoch 219: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 123s 604ms/step - loss: 0.4139 - accuracy: 0.8355 - val_loss: 0.4175 - val_accuracy: 0.8271\n",
      "Epoch 220/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8358\n",
      "Epoch 220: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4083 - accuracy: 0.8358 - val_loss: 0.4097 - val_accuracy: 0.8372\n",
      "Epoch 221/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.8360\n",
      "Epoch 221: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 121s 595ms/step - loss: 0.4071 - accuracy: 0.8360 - val_loss: 0.4001 - val_accuracy: 0.8414\n",
      "Epoch 222/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8426\n",
      "Epoch 222: val_loss did not improve from 0.39898\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4036 - accuracy: 0.8426 - val_loss: 0.4014 - val_accuracy: 0.8411\n",
      "Epoch 223/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4116 - accuracy: 0.8326\n",
      "Epoch 223: val_loss improved from 0.39898 to 0.39807, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 121s 593ms/step - loss: 0.4116 - accuracy: 0.8326 - val_loss: 0.3981 - val_accuracy: 0.8364\n",
      "Epoch 224/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4114 - accuracy: 0.8375\n",
      "Epoch 224: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 121s 592ms/step - loss: 0.4114 - accuracy: 0.8375 - val_loss: 0.4081 - val_accuracy: 0.8357\n",
      "Epoch 225/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.8391\n",
      "Epoch 225: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4065 - accuracy: 0.8391 - val_loss: 0.4140 - val_accuracy: 0.8382\n",
      "Epoch 226/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8412\n",
      "Epoch 226: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 120s 591ms/step - loss: 0.4045 - accuracy: 0.8412 - val_loss: 0.4072 - val_accuracy: 0.8443\n",
      "Epoch 227/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8371\n",
      "Epoch 227: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 120s 587ms/step - loss: 0.4085 - accuracy: 0.8371 - val_loss: 0.4029 - val_accuracy: 0.8414\n",
      "Epoch 228/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.8365\n",
      "Epoch 228: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 120s 590ms/step - loss: 0.4066 - accuracy: 0.8365 - val_loss: 0.4167 - val_accuracy: 0.8218\n",
      "Epoch 229/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4098 - accuracy: 0.8388\n",
      "Epoch 229: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 122s 597ms/step - loss: 0.4098 - accuracy: 0.8388 - val_loss: 0.4273 - val_accuracy: 0.8261\n",
      "Epoch 230/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.8397\n",
      "Epoch 230: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.4047 - accuracy: 0.8397 - val_loss: 0.4050 - val_accuracy: 0.8325\n",
      "Epoch 231/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8455\n",
      "Epoch 231: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.3986 - accuracy: 0.8455 - val_loss: 0.4023 - val_accuracy: 0.8379\n",
      "Epoch 232/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8349\n",
      "Epoch 232: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 123s 603ms/step - loss: 0.4128 - accuracy: 0.8349 - val_loss: 0.4043 - val_accuracy: 0.8354\n",
      "Epoch 233/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.8394\n",
      "Epoch 233: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 134s 659ms/step - loss: 0.4049 - accuracy: 0.8394 - val_loss: 0.4144 - val_accuracy: 0.8271\n",
      "Epoch 234/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8431\n",
      "Epoch 234: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 127s 622ms/step - loss: 0.4004 - accuracy: 0.8431 - val_loss: 0.4157 - val_accuracy: 0.8282\n",
      "Epoch 235/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8421\n",
      "Epoch 235: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 124s 606ms/step - loss: 0.4020 - accuracy: 0.8421 - val_loss: 0.4127 - val_accuracy: 0.8289\n",
      "Epoch 236/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4048 - accuracy: 0.8378\n",
      "Epoch 236: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 124s 609ms/step - loss: 0.4048 - accuracy: 0.8378 - val_loss: 0.4108 - val_accuracy: 0.8343\n",
      "Epoch 237/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4057 - accuracy: 0.8418\n",
      "Epoch 237: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 133s 654ms/step - loss: 0.4057 - accuracy: 0.8418 - val_loss: 0.4079 - val_accuracy: 0.8379\n",
      "Epoch 238/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4103 - accuracy: 0.8335\n",
      "Epoch 238: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 153s 753ms/step - loss: 0.4103 - accuracy: 0.8335 - val_loss: 0.4155 - val_accuracy: 0.8354\n",
      "Epoch 239/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4043 - accuracy: 0.8398\n",
      "Epoch 239: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 127s 626ms/step - loss: 0.4043 - accuracy: 0.8398 - val_loss: 0.4136 - val_accuracy: 0.8343\n",
      "Epoch 240/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4048 - accuracy: 0.8380\n",
      "Epoch 240: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 581ms/step - loss: 0.4048 - accuracy: 0.8380 - val_loss: 0.4147 - val_accuracy: 0.8271\n",
      "Epoch 241/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.8431\n",
      "Epoch 241: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 580ms/step - loss: 0.4050 - accuracy: 0.8431 - val_loss: 0.4257 - val_accuracy: 0.8339\n",
      "Epoch 242/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.8400\n",
      "Epoch 242: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 119s 584ms/step - loss: 0.4046 - accuracy: 0.8400 - val_loss: 0.4055 - val_accuracy: 0.8343\n",
      "Epoch 243/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.8388\n",
      "Epoch 243: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 581ms/step - loss: 0.4074 - accuracy: 0.8388 - val_loss: 0.4198 - val_accuracy: 0.8182\n",
      "Epoch 244/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8417\n",
      "Epoch 244: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 579ms/step - loss: 0.4055 - accuracy: 0.8417 - val_loss: 0.4057 - val_accuracy: 0.8393\n",
      "Epoch 245/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8449\n",
      "Epoch 245: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 578ms/step - loss: 0.3986 - accuracy: 0.8449 - val_loss: 0.4159 - val_accuracy: 0.8278\n",
      "Epoch 246/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.8386\n",
      "Epoch 246: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 578ms/step - loss: 0.4011 - accuracy: 0.8386 - val_loss: 0.4081 - val_accuracy: 0.8393\n",
      "Epoch 247/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.8368\n",
      "Epoch 247: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 578ms/step - loss: 0.4061 - accuracy: 0.8368 - val_loss: 0.4084 - val_accuracy: 0.8318\n",
      "Epoch 248/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.8454\n",
      "Epoch 248: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 117s 574ms/step - loss: 0.3948 - accuracy: 0.8454 - val_loss: 0.3993 - val_accuracy: 0.8465\n",
      "Epoch 249/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.8381\n",
      "Epoch 249: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 580ms/step - loss: 0.4056 - accuracy: 0.8381 - val_loss: 0.4030 - val_accuracy: 0.8422\n",
      "Epoch 250/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3999 - accuracy: 0.8440\n",
      "Epoch 250: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 117s 576ms/step - loss: 0.3999 - accuracy: 0.8440 - val_loss: 0.4116 - val_accuracy: 0.8343\n",
      "Epoch 251/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3980 - accuracy: 0.8415\n",
      "Epoch 251: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 577ms/step - loss: 0.3980 - accuracy: 0.8415 - val_loss: 0.4124 - val_accuracy: 0.8311\n",
      "Epoch 252/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.8426\n",
      "Epoch 252: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 117s 576ms/step - loss: 0.3971 - accuracy: 0.8426 - val_loss: 0.4387 - val_accuracy: 0.8318\n",
      "Epoch 253/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8358\n",
      "Epoch 253: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 578ms/step - loss: 0.4055 - accuracy: 0.8358 - val_loss: 0.4074 - val_accuracy: 0.8307\n",
      "Epoch 254/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8388\n",
      "Epoch 254: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 582ms/step - loss: 0.3969 - accuracy: 0.8388 - val_loss: 0.4078 - val_accuracy: 0.8289\n",
      "Epoch 255/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8454\n",
      "Epoch 255: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 580ms/step - loss: 0.3945 - accuracy: 0.8454 - val_loss: 0.4007 - val_accuracy: 0.8432\n",
      "Epoch 256/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.8418\n",
      "Epoch 256: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 577ms/step - loss: 0.3963 - accuracy: 0.8418 - val_loss: 0.4076 - val_accuracy: 0.8311\n",
      "Epoch 257/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8431\n",
      "Epoch 257: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 118s 579ms/step - loss: 0.3926 - accuracy: 0.8431 - val_loss: 0.4006 - val_accuracy: 0.8457\n",
      "Epoch 258/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8435\n",
      "Epoch 258: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 125s 615ms/step - loss: 0.3970 - accuracy: 0.8435 - val_loss: 0.4044 - val_accuracy: 0.8346\n",
      "Epoch 259/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3957 - accuracy: 0.8395\n",
      "Epoch 259: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 122s 598ms/step - loss: 0.3957 - accuracy: 0.8395 - val_loss: 0.4003 - val_accuracy: 0.8386\n",
      "Epoch 260/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.8317\n",
      "Epoch 260: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 130s 638ms/step - loss: 0.4050 - accuracy: 0.8317 - val_loss: 0.4090 - val_accuracy: 0.8386\n",
      "Epoch 261/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8441\n",
      "Epoch 261: val_loss did not improve from 0.39807\n",
      "204/204 [==============================] - 127s 625ms/step - loss: 0.3959 - accuracy: 0.8441 - val_loss: 0.4065 - val_accuracy: 0.8311\n",
      "Epoch 262/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.8450\n",
      "Epoch 262: val_loss improved from 0.39807 to 0.39722, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 133s 654ms/step - loss: 0.3942 - accuracy: 0.8450 - val_loss: 0.3972 - val_accuracy: 0.8411\n",
      "Epoch 263/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3954 - accuracy: 0.8395\n",
      "Epoch 263: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 126s 619ms/step - loss: 0.3954 - accuracy: 0.8395 - val_loss: 0.4359 - val_accuracy: 0.8096\n",
      "Epoch 264/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8403\n",
      "Epoch 264: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 125s 614ms/step - loss: 0.4028 - accuracy: 0.8403 - val_loss: 0.3982 - val_accuracy: 0.8450\n",
      "Epoch 265/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8440\n",
      "Epoch 265: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 125s 614ms/step - loss: 0.3998 - accuracy: 0.8440 - val_loss: 0.4067 - val_accuracy: 0.8400\n",
      "Epoch 266/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8455\n",
      "Epoch 266: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 125s 614ms/step - loss: 0.3945 - accuracy: 0.8455 - val_loss: 0.4054 - val_accuracy: 0.8346\n",
      "Epoch 267/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8450\n",
      "Epoch 267: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 126s 617ms/step - loss: 0.3958 - accuracy: 0.8450 - val_loss: 0.4068 - val_accuracy: 0.8404\n",
      "Epoch 268/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.8409\n",
      "Epoch 268: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 125s 615ms/step - loss: 0.3994 - accuracy: 0.8409 - val_loss: 0.4415 - val_accuracy: 0.8200\n",
      "Epoch 269/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.8490\n",
      "Epoch 269: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 125s 614ms/step - loss: 0.3856 - accuracy: 0.8490 - val_loss: 0.4058 - val_accuracy: 0.8354\n",
      "Epoch 270/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8368\n",
      "Epoch 270: val_loss did not improve from 0.39722\n",
      "204/204 [==============================] - 126s 616ms/step - loss: 0.4041 - accuracy: 0.8368 - val_loss: 0.4168 - val_accuracy: 0.8325\n",
      "Epoch 271/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8383\n",
      "Epoch 271: val_loss improved from 0.39722 to 0.39479, saving model to best_cnn1_MobilNet.h5\n",
      "204/204 [==============================] - 127s 621ms/step - loss: 0.3997 - accuracy: 0.8383 - val_loss: 0.3948 - val_accuracy: 0.8447\n",
      "Epoch 272/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8418\n",
      "Epoch 272: val_loss did not improve from 0.39479\n",
      "204/204 [==============================] - 124s 611ms/step - loss: 0.3970 - accuracy: 0.8418 - val_loss: 0.4007 - val_accuracy: 0.8379\n",
      "Epoch 273/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8466\n",
      "Epoch 273: val_loss did not improve from 0.39479\n",
      "204/204 [==============================] - 126s 619ms/step - loss: 0.3941 - accuracy: 0.8466 - val_loss: 0.4089 - val_accuracy: 0.8339\n",
      "Epoch 274/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.8386\n",
      "Epoch 274: val_loss did not improve from 0.39479\n",
      "204/204 [==============================] - 125s 616ms/step - loss: 0.3993 - accuracy: 0.8386 - val_loss: 0.4053 - val_accuracy: 0.8332\n",
      "Epoch 275/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8452\n",
      "Epoch 275: val_loss did not improve from 0.39479\n",
      "204/204 [==============================] - 118s 581ms/step - loss: 0.3931 - accuracy: 0.8452 - val_loss: 0.4136 - val_accuracy: 0.8393\n",
      "Epoch 276/400\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8403\n",
      "Epoch 276: val_loss did not improve from 0.39479\n",
      "204/204 [==============================] - 119s 582ms/step - loss: 0.4003 - accuracy: 0.8403 - val_loss: 0.4033 - val_accuracy: 0.8375\n",
      "Epoch 277/400\n",
      "173/204 [========================>.....] - ETA: 13s - loss: 0.4006 - accuracy: 0.8383"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\josei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    callbacks = [early_stopping, model_checkpoint],\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {0: .4f}. Test accuracy: {1: .2f}%'.format(test_loss, test_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn1_MobilNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "best_model = load_model('best_cnn1_MobilNet.h5')\n",
    "\n",
    "# Hacer predicciones con el mejor modelo\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {0: .4f}. Test accuracy: {1: .2f}%'.format(test_loss, test_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Supongamos que tienes un historial llamado 'history' con registros de prdida y precisin para entrenamiento y validacin.\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Crear subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Grfico para prdida\n",
    "plt.sca(axes[0])\n",
    "sns.lineplot(x=epochs, y=train_loss, label='Train Loss')\n",
    "sns.lineplot(x=epochs, y=val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Grfico para precisin\n",
    "plt.sca(axes[1])\n",
    "sns.lineplot(x=epochs, y=train_accuracy, label='Train Accuracy')\n",
    "sns.lineplot(x=epochs, y=val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Ajusta la disposicin\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra la figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tienes las etiquetas reales (y_true) y las predicciones (y_pred).\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)\n",
    "\n",
    "class_labels = [\"bcc\", \"scc\"]\n",
    "\n",
    "# Crear un mapa de calor para la matriz de confusin\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Supongamos que tienes etiquetas reales (y_true) y probabilidades de prediccin (y_probs).\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Crear un grfico de la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=fpr, y=tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"D:/SkinCancerDatasets/dataset/errors/\"+tipo+\"/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for image, true_label, predicted_label in zip(x_test, y_test, y_pred):\n",
    "    if true_label != predicted_label:\n",
    "        if predicted_label == 1:\n",
    "            label_name = \"bcc\"\n",
    "        else:\n",
    "            label_name = \"scc\"\n",
    "    \n",
    "        # Crea un directorio para la etiqueta si no existe\n",
    "        label_directory = os.path.join(output_directory, label_name)\n",
    "        os.makedirs(label_directory, exist_ok=True)\n",
    "    \n",
    "        # Genera un nombre de archivo nico para la imagen\n",
    "        image_name = f\"image_{len(os.listdir(label_directory))}.jpg\"\n",
    "        output_path = os.path.join(label_directory, image_name)\n",
    "    \n",
    "        # Guarda la imagen en la carpeta de la etiqueta\n",
    "        cv.imwrite(output_path, image * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monto_mensual = 20000\n",
    "years = 10\n",
    "interes_anual = 0.10\n",
    "\n",
    "tiempo = years * 12\n",
    "total = 0\n",
    "\n",
    "for i in range(tiempo):\n",
    "    total = total + monto_mensual\n",
    "    total = total * (1 + interes_anual/12)\n",
    "\n",
    "print(monto_mensual*tiempo)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
